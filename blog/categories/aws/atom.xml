<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Aws | Coding Forest]]></title>
  <link href="http://jivimberg.github.io/blog/categories/aws/atom.xml" rel="self"/>
  <link href="http://jivimberg.github.io/"/>
  <updated>2020-07-19T20:20:42-07:00</updated>
  <id>http://jivimberg.github.io/</id>
  <author>
    <name><![CDATA[Juan Ignacio Vimberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Working With Queues]]></title>
    <link href="http://jivimberg.github.io/blog/2020/05/30/working-with-queues/"/>
    <updated>2020-05-30T19:29:39-07:00</updated>
    <id>http://jivimberg.github.io/blog/2020/05/30/working-with-queues</id>
    <content type="html"><![CDATA[<p>Queues are a powerful tool for building reliable systems. In this article, I’ll describe some of the tips and tricks I came across when working with queues.</p>

<p>Some of the advice is specific to Amazon SQS queues because that’s what I’ve been using the most lately. And also because some of them come from <a href="https://aws.amazon.com/builders-library/avoiding-insurmountable-queue-backlogs/">this amazing article</a> from the <a href="https://aws.amazon.com/builders-library/">Amazon Builders’ Library</a>.</p>

<!--more-->


<h2>The trade-off</h2>

<p>Queues can be used to increase the system’s availability by accepting new messages even if our service is down. They help us decouple producers from consumers. When using systems like SQS, we also get a durability guarantee, because we know messages published won’t be lost if our system fails as they are persisted in the queue. Additionally, we get an increase in reliability since we can configure our system to retry the processing of a message in case of failure.</p>

<p><img class="center" src="/images/posts/2020-06-07/Availability.png" title="‘Example of how queues can increase availability’" ></p>

<p>These advantages come at a cost. <strong>We get better reliability, availability, and durability at the price of increased latency</strong>. Meaning, messages can take longer to be processed compared to a synchronous system. This is because our system might have to go through a backlog of old messages before getting to the one just published. Furthermore, if the pace at which messages are put on the queue is faster than the speed at which our system can process them, the system might never be able to catch up!</p>

<p><img class="center" src="/images/posts/2020-06-07/Overflow.gif" title="‘Animation on how a slow consuming queue can overflow’" ></p>

<p>Let’s go over some of the things we can do to prevent or mitigate these risks.</p>

<h3>1. Wrapping your queues</h3>

<p>Instead of exposing the queues to clients, you can wrap them with an ordering API. This way, we maintain more control over what’s published in the queue. Wrapping queues have many benefits:</p>

<ol>
<li>We can run validations over the message payload and reject malformed messages with an appropriate error.</li>
<li>We can enrich the message payload with caller information.</li>
<li>We can authenticate callers to control access.</li>
<li>We can implement some of the patterns mentioned below to control fairness in a multi-tenant system and handle surges.</li>
</ol>


<p><img class="center" src="/images/posts/2020-06-07/WrapQueues.png" title="‘System diagram for wrapping queues’" ></p>

<p>The downside of wrapping the queues is that we turn an asynchronous call into synchronous. Now our system has to be up to process new messages. We’re trading the availability improvements for more control.</p>

<h3>2. Dealing with a backlog</h3>

<p>The price of increased availability is having to deal with the backlog of messages that occur in a surge or after a failure. One way to do so is by dropping old messages.  When consuming a new message, we can compare the current time with the time the message was published and discard the message if it is greater than some value. Of course, this only works if the systems can tolerate this type of message loss.</p>

<p>Another technique is to move the excess to a spillover queue to be processed later. The system will first work on the new messages on the main queue, and only tackle the ones on the spillover queue once resources are available. This way, we can approximate LIFO order, which might be more appropriate for systems dealing with real-time events.</p>

<p><img class="center" src="/images/posts/2020-06-07/Spillover.png" title="‘Diagram of a system using spillover queue’" ></p>

<p>Finally, we can measure the size of the backlog and scale the number of consumers accordingly. Once the backlog is back to its normal size, we can scale down the consumer instances.</p>

<h3>3. Ensuring fairness</h3>

<p>One of the challenges of having multiple customers is having to guarantee fairness. That is, making sure one client is not exhausting all the available resources, creating significant latencies on other clients’ messages. This is especially true in multi-tenant environments where clients might not be aware they’re sharing resources with other people.</p>

<p>One possible solution is to have different customers publish to different queues, and have the system consume in a round-robin fashion. This is a simple solution, but it does not scale well. If we had thousands of customers, we’d have to manage and poll thousands of queues. Instead, we can have a fixed number of queues and hash each customer to a small number of them. Whenever we receive a message, we retrieve the queues assigned to that customer and put the message on the queue with the shortest backlog. That way, if a client is producing lots of messages on their queues, other workflows are automatically routed to less utilized queues. One caveat worth considering, is that message order is not preserved in this model.</p>

<p><img class="center" src="/images/posts/2020-06-07/Sharding.gif" title="‘Animation of multi-tenant system using sharing’" ></p>

<p>Another solution is to set a rate for messages processed for each customer. Once the customer has gone over the specified rate, messages are put in a spillover queue to be deal with later. This pattern is similar to the one we applied for old messages in the previous section, only in this case we’re using it to prevent one client from exhausting all the processing power.</p>

<h3>3. Ensure enough capacity for surges</h3>

<p>It is crucial to reserve additional resources to be able to handle spikes in traffic. One smart idea is to measure the number of messages retrieved while polling. If the system is retrieving more messages on every poll attempt, it means we probably don’t have enough spare resources to handle a surge.</p>

<h3>4. Updating the visibility timeout</h3>

<p>The way Amazon SQS works is that whenever a consumer receives a message, the message remains on the queue hidden. Other consumers won’t be able to see the message for a period of time known as <em>visibilityTimeout</em>. Once the <em>visibilityTimeout</em> period is up, if the message has not been deleted from the queue, other consumers will be able to get it and process it.</p>

<p>If processing a message is taking too long, we run the risk of going over the <em>visibilityTimeout</em> period. If that happens, another client will receive the message and start churning away, spending more resources on it, even though the first consumer has a better chance of finishing first. To avoid this, when we realize processing is taking too long, we can heartbeat SQS to let it know we’re still working. We do this by updating the <em>visibilityTimeout</em> period for a particular message.</p>

<p>We can also use the ability to programmatically modify the <em>visibilityTimeout</em> for a message to speed up retries. Say our queue is configured with a <em>visibilityTimeout</em> of 10 minutes, and while processing a message, we face a transient error, we can set <em>visibilityTimeout</em> to zero to make it retry faster.</p>

<p> <img class="right-fill" src="/images/signatures/signature8.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SQS Consumer Using Kotlin Coroutines]]></title>
    <link href="http://jivimberg.github.io/blog/2019/02/23/sqs-consumer-using-kotlin-coroutines/"/>
    <updated>2019-02-23T16:52:48-03:00</updated>
    <id>http://jivimberg.github.io/blog/2019/02/23/sqs-consumer-using-kotlin-coroutines</id>
    <content type="html"><![CDATA[<p>Today we’ll see how to write a SQS consumer that processes messages in a parallel, non-blocking way, using Kotlin coroutines.</p>

<!--more-->


<h2>The pool of workers pattern</h2>

<p>After some experimentation I’ve opted for using a <em>pool of workers</em> for writing the consumer. For an introduction on the pattern and how to implement it in Kotlin I strongly suggested watching Roman’s talk from 2018 Kotlin Conf.</p>

<div style="text-align: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/a3agLJQ6vt8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>


<p>For our particular solution we’ll need:</p>

<ol>
<li>One coroutine that periodically retrieves the messages (<strong>MsgReceiver</strong>).</li>
<li>Multiple <strong>workers</strong> that process the receiving messages in parallel without blocking.</li>
<li>A <strong>channel</strong> to communicate between the <em>MsgReceiver</em> coroutine and the <em>Workers</em>.</li>
</ol>


<p><img class="center" src="/images/posts/2019-03-10/consumerDiagram.png" width="800" title="‘SQS consumer diagram’" ></p>

<h3><em>fun start()</em></h3>

<p>Let’s start by creating the elements we mentioned in the previous section. We’ll do this in the <code>start()</code> method.</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
fun start() = launch {
        val messageChannel = Channel<Message>()
        repeat(N_WORKERS) { launchWorker(messageChannel) }
        launchMsgReceiver(messageChannel)
    }
</xmp>


<p>This function launches a coroutine that creates: the channel, <em>N</em> <em>workers</em> and the <em>MsgReceiver</em>.</p>

<p>Don’t worry too much about how we’re able to call <code>launch</code> here. I’ll go back to this later.</p>

<h3>Message receiver</h3>

<p>Now it’s time to write the code for the message receiver:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
private fun CoroutineScope.launchMsgReceiver(channel: SendChannel<Message>) = launch {
        repeatUntilCancelled {
            val receiveRequest = ReceiveMessageRequest.builder()
                    .queueUrl(SQS_URL)
                    .waitTimeSeconds(20)
                    .maxNumberOfMessages(10)
                    .build()
    
            val messages = sqs.receiveMessage(receiveRequest).await().messages()
            println("${Thread.currentThread().name} Retrieved ${messages.size} messages")
    
            messages.forEach {
                channel.send(it)
            }
        }
    }
</xmp>


<p>This function takes a <code>SendChannel</code> that it’ll use to communicate with the <em>worker</em> coroutines.</p>

<p>It is written as a <code>CoroutineScope</code> to denote that it creates a new coroutine and does not wait for it to complete.</p>

<p>Next thing you’ll notice is that all the code is wrapped with a <code>repeatUntilCancelled</code>. This is a helper function that will repeat our block infinitely, and make sure it keeps running even in the face of exceptions. It will only stop when the coroutine is cancelled. We’ll use this little trick on our <em>Worker</em> code too. Here’s the code, courtesy of <a href="https://twitter.com/_fletchr">@_fletchr</a>:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
import kotlinx.coroutines.CancellationException
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.isActive
import kotlinx.coroutines.yield
import java.lang.Thread.currentThread

//sampleStart
suspend fun CoroutineScope.repeatUntilCancelled(block: suspend () -> Unit) {
    while (isActive) {
        try {
            block()
            yield()
        } catch (ex: CancellationException) {
            println("coroutine on ${currentThread().name} cancelled")
        } catch (ex: Exception) {
            println("${currentThread().name} failed with {$ex}. Retrying...")
            ex.printStackTrace()
        }
    }
    
    println("coroutine on ${currentThread().name} exiting")
}
//sampleEnd
</xmp>


<p>We have while loop on the <code>isActive</code> property from the CoroutineScope that will guarantee we keep repeating the block until the coroutine is no longer active (i.e. it has been cancelled). We have access to this property because <code>repeatUntilCancelled</code> is an extension function on CoroutineScope.</p>

<p>Having the <em>try/catch</em> guarantees the coroutine will keep looping even if the block throws an exception (like a connection timeout reading messages from the queue, for example). <code>CancellationException</code> is the exception used by the coroutines machinery to signal the cancellation of the coroutine. We handle it as a special case because we don&rsquo;t want to print a message saying <em>&ldquo;Retrying&hellip;&rdquo;</em>, because the while loop is about to break.</p>

<p>The call to <code>yield()</code> is needed to avoid the case where all threads are busy with CPU intensive coroutines that do not suspend, and thus there&rsquo;s no chance for other coroutines to execute.</p>

<p>Ok, back to our <code>launchMsgReceiver</code> function!  The next part is the actual polling for messages. You’ll notice that I’m maxing out the <code>waitTimeSeconds</code>  so the call waits up to <em>20 seconds</em>, for at least one message to be available before returning empty. I’m also picking the biggest value for the number of messages being retrieved at once (10), because the whole point of my consumer is to be able to process as many of them in parallel, as possible.</p>

<p>Next line is the <code>receiveMessage</code> call on the SQS client. One extremely important detail is that <strong>I’m using the <code>SqsAsyncClient</code> from <a href="https://github.com/aws/aws-sdk-java-v2">AWS Java SDK v2</a></strong>. Why? Because I don’t want my thread to be blocked waiting for messages to appear. And the <code>SqsAsyncClient</code> of Java SKD v1 returns only a <code>Future</code> (because it needs to be compatible with Java 1.6) making it harder to integrate with the coroutines world. Instead, the new version returns <code>CompletableFuture</code>, which lets us call <code>await()</code> (from <a href="https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-jdk8/">Kotlinx Coroutines JDK8</a>) to suspend our coroutine until the result is ready <strong>without blocking the thread</strong>.</p>

<p>Finally we iterate over the messages and send them through the channel using  <code>channel.send(it)</code> . Because of the way <a href="https://kotlinlang.org/docs/reference/coroutines/channels.html">unbuffered channels</a> work the <code>send</code> call will suspend if there is no worker available to receive the message. This is a very nice property because it means <strong>we get <a href="https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7"><em>backpressure</em></a> for free</strong>. If at some point our workers are not able to process the messages fast enough, the <em>MsgReceiver</em> will just wait (suspend) until some worker becomes available, instead of fetching even more messages, drowning the workers.</p>

<h3>Worker</h3>

<p>Now let’s take a look at the worker that will process the messages:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
 private fun CoroutineScope.launchWorker(channel: ReceiveChannel<Message>) = launch {
        repeatUntilCancelled {
            for (msg in channel) {
                try {
                    processMsg(msg)
                    deleteMessage(msg)
                } catch (ex: Exception) {
                    println("${Thread.currentThread().name} exception trying to process message ${msg.body()}")
                    ex.printStackTrace()
                    changeVisibility(msg)
                }
            }
        }
    }
</xmp>


<p>The first lines are pretty similar to the <em>MsgReceiver</em>. We’re again launching a coroutine and not waiting for it to complete, and thus we have the <code>launch</code> call and our function is an extension of <code>CoroutineScope</code>. We’re also wrapping everything in a <code>repeatUntilCancelled</code> for the same reasons we used it before. The only difference you might notice is that it takes a <code>ReceiveChannel</code> instead of <code>SendChannel</code>, because this is the receiving end of the communication.</p>

<p>Next we use a <code>for</code> loop to consume messages from the channel. It will suspend the coroutine if there are no new messages in the channel. The use of <code>for</code> is important because we’re doing <a href="https://kotlinlang.org/docs/reference/coroutines/channels.html#fan-out">fan-out</a>, we have multiple coroutines consuming from the same channel. Consuming with <code>for</code> guarantees us that if one coroutine fails it won’t cancel the underlying channel, which is what would happen if we had used <code>consumeEach</code> to go through the messages, instead.</p>

<p>Also channels have the nice property of <a href="https://kotlinlang.org/docs/reference/coroutines/channels.html#channels-are-fair">being fair</a>. Meaning that the first coroutine that invoke <code>receive</code> gets the message. FIFO style.</p>

<p>If something fails while processing the message we (try to) <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html">change the visibility timeout</a> so that the message will show up in the queue again sooner, and picked up for re-processing.</p>

<p>Let’s now take a look at <em>processMessage(msg)</em> implementation:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
private suspend fun processMsg(message: Message) {
        println("${Thread.currentThread().name} Started processing message: ${message.body()}")
        delay((1000L..2000L).random())
        println("${Thread.currentThread().name} Finished processing of message: ${message.body()}")
    }
</xmp>


<p>As you can tell, this is a mock implementation. There’s nothing interesting happening here. We just <code>delay</code> for a few seconds and then continue saying that message has been processed. This is where you’d put your actual logic. Or better yet, if you plan to reuse your SQS consumer implementation <strong>you might want to turn this into a Lambda expression that can be passed as a parameter</strong>.</p>

<p>Finally let’s see <em>deleteMsg(msg)</em> and <em>changeVisibility(msg)</em>:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
private suspend fun deleteMessage(message: Message) {
        sqs.deleteMessage { req ->
            req.queueUrl(SQS_URL)
            req.receiptHandle(message.receiptHandle())
        }.await()
        println("${Thread.currentThread().name} Message deleted: ${message.body()}")
    }
    
    private suspend fun changeVisibility(message: Message) {
        sqs.changeMessageVisibility { req ->
            req.queueUrl(SQS_URL)
            req.receiptHandle(message.receiptHandle())
            req.visibilityTimeout(10)
        }.await()
        println("${Thread.currentThread().name} Changed visibility of message: ${message.body()}")
    }
</xmp>


<p>This 2 methods follow the same pattern: call the <code>SQSAsyncClient</code> corresponding method then <code>await()</code>, finally log.</p>

<h3>Contexts, dispatchers and supervisors</h3>

<p>Now that we have a good grasp on the different parts of our solution, let’s pay closer attention to how the coroutines are created and dispatched.</p>

<p>One of the central tenets of Kotlin coroutines is <em><a href="https://kotlinlang.org/docs/reference/coroutines/basics.html#structured-concurrency">structured concurrency</a></em>. <em>Structured concurrency</em> helps us write code that properly cleans up active coroutines in case of exceptions. If you think you’re a hardcore developer and instead launch your coroutines using <code>GlobalScope.launch { ... }</code>, then you risk leaking coroutines when something fails (check <a href="https://twitter.com/relizarov">@relizarov</a>’s <a href="https://medium.com/@elizarov/structured-concurrency-722d765aa952">article</a> to see an example of this). So, how’s are our coroutines structured?</p>

<p><img class="center" src="/images/posts/2019-03-10/coroutinesScope.png" width="400" title="‘coroutines structure’" ></p>

<p>To mark the lifecycle of our coroutines we extend our class with <code>CoroutineScope</code> and provide a context by overriding <code>corroutinesContext: CorroutineContext</code>. This is the context that’ll be used by our root coroutine, the one started by the <code>launch</code> call on the <code>start()</code> function, at the beginning of this post. Since we want our coroutines to match the lifecycle of our class we’ve added a <code>close()</code> method that clients can call to cancel all the coroutines in the consumer. If you’re using some kind of framework you might want to tie this function the lifecycle of the class in some other way.</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
package com.jivimberg.sqs

import kotlinx.coroutines.\*
import kotlinx.coroutines.channels.Channel
import kotlinx.coroutines.channels.ReceiveChannel
import kotlinx.coroutines.channels.SendChannel
import kotlinx.coroutines.future.await
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.sqs.SqsAsyncClient
import software.amazon.awssdk.services.sqs.model.Message
import software.amazon.awssdk.services.sqs.model.ReceiveMessageRequest
import kotlin.coroutines.CoroutineContext

//sampleStart

class SqsSampleConsumerChannels(
        private val sqs: SqsAsyncClient
) : CoroutineScope {

    private val supervisorJob = SupervisorJob()
    override val coroutineContext: CoroutineContext
        get() = Dispatchers.IO + supervisorJob
    
    fun start() = launch {
        val messageChannel = Channel<Message>()
        repeat(N_WORKERS) { launchWorker(messageChannel) }
        launchMsgReceiver(messageChannel)
    }
    
    fun stop() {
        supervisorJob.cancel()
    }

//sampleEnd

    private fun CoroutineScope.launchMsgReceiver(channel: SendChannel<Message>) = launch {
        repeatUntilCancelled {
            val receiveRequest = ReceiveMessageRequest.builder()
                    .queueUrl(SQS_URL)
                    .waitTimeSeconds(20)
                    .maxNumberOfMessages(10)
                    .build()
    
            val messages = sqs.receiveMessage(receiveRequest).await().messages()
            println("${Thread.currentThread().name} Retrieved ${messages.size} messages")
    
            messages.forEach {
                channel.send(it)
            }
        }
    }
    
    private fun CoroutineScope.launchWorker(channel: ReceiveChannel<Message>) = launch {
        repeatUntilCancelled {
            for (msg in channel) {
                try {
                    processMsg(msg)
                    deleteMessage(msg)
                } catch (ex: Exception) {
                    println("${Thread.currentThread().name} exception trying to process message ${msg.body()}")
                    ex.printStackTrace()
                    changeVisibility(msg)
                }
            }
        }
    }
    
    private suspend fun processMsg(message: Message) {
        println("${Thread.currentThread().name} Started processing message: ${message.body()}")
        delay((1000L..2000L).random())
        println("${Thread.currentThread().name} Finished processing of message: ${message.body()}")
    }
    
    private suspend fun deleteMessage(message: Message) {
        sqs.deleteMessage { req ->
            req.queueUrl(SQS_URL)
            req.receiptHandle(message.receiptHandle())
        }.await()
        println("${Thread.currentThread().name} Message deleted: ${message.body()}")
    }
    
    private suspend fun changeVisibility(message: Message) {
        sqs.changeMessageVisibility { req ->
            req.queueUrl(SQS_URL)
            req.receiptHandle(message.receiptHandle())
            req.visibilityTimeout(10)
        }.await()
        println("${Thread.currentThread().name} Changed visibility of message: ${message.body()}")
    }
}

fun main() = runBlocking {
    println("${Thread.currentThread().name} Starting program")
    val sqs = SqsAsyncClient.builder()
            .region(Region.US_EAST_1)
            .build()
    val consumer = SqsSampleConsumerChannels(sqs)
    consumer.start()
    delay(30000)
    consumer.stop()
}

private const val N\_WORKERS = 4
</xmp>


<p>Our <code>coroutineContext</code> has 2 elements: the <code>Dispatcher</code> and the <code>supervisorJob</code> (<a href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.coroutines.experimental/-coroutine-context/plus.html">the <code>+</code> operation</a> concatenates both elements in a new <code>CoroutineContext</code> ).</p>

<p>The <em>Dispatcher</em> marks which thread our coroutine will run on. In our case I used <code>Dispatchers.IO</code> that uses a shared pool of threads for doing IO (more details <a href="https://kotlin.github.io/kotlinx.coroutines/kotlinx-coroutines-core/kotlinx.coroutines/-dispatchers/-i-o.html">here</a>). This is because most of the time my coroutines are doing IO operations for: receiving messages, deleting them, changing visibility, etc. If, in your case, processing a message requires a CPU intensive calculation you’d use <code>Dispatchers.Default</code> through the <code>withContext</code> function like this:</p>

<xmp class="kotlin-code" data-highlight-only theme="darcula">
private suspend fun processMsg(message: Message) = withContext(Dispatchers.Default) {
        // Your code here
    }
</xmp>


<h2>In case of exception</h2>

<p>Because we’re responsible developers, let’s see what would happen if something were to fail.</p>

<p>As a first line of defense we know both our <em>Workers</em> and the <em>MsgReceiver</em> are running in <code>repeatUntilCancelled</code> loops, so any exception should be logged and ignored. This is ok for us because once the visibility timeout is over, the message will re-appear in the queue for another worker to consume. Still, to be extra careful, let’s consider what would happen with our coroutines in case of exception if we didn’t have <code>repeatUntilCancelled</code>s.</p>

<p>The whole premise of <a href="https://kotlinlang.org/docs/reference/coroutines/basics.html#structured-concurrency">structured concurrency</a> is to avoid leaking coroutines in case of failure. That’s why, by default, <strong>whenever a child coroutine terminates with exception its parent and siblings are cancelled too</strong>. This is a nice property, but not quite what we want in this case. We don’t want all workers to be cancelled because one worker threw an exception. That’s why we use <code>supervisorJob</code> as part of our <code>coroutineContext</code>. <a href="https://kotlinlang.org/docs/reference/coroutines/exception-handling.html#supervision-job">SupervisorJob</a> is like a regular job, with the exception that cancellation is propagated only downwards.</p>

<p><img class="center" src="/images/posts/2019-03-10/Supervisor.png" width="600" title="‘Regular Job vs. Supervisor Job’" ></p>

<p>Finally, don’t forget to configure a <a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html">dead letter queue</a> for those messages that can’t be processed even after multiple retries.</p>

<hr />

<p>That’s all! You can find all the code from this post in <a href="https://gist.github.com/jivimberg/b0f4f94871c6f3e7d17fae1106c28047">this gist</a>. If you have any improvement to suggest, I’d ❤️ to hear about it. Leave me a comment down there.</p>

<p><img class="right-fill" src="/images/signatures/signature11.png" width="200" title="‘My signature’" ></p>

<script src="https://unpkg.com/kotlin-playground@1" data-selector=".kotlin-code"></script>



]]></content>
  </entry>
  
</feed>
