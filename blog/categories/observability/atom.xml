<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Observability | Coding Forest]]></title>
  <link href="http://jivimberg.github.io/blog/categories/observability/atom.xml" rel="self"/>
  <link href="http://jivimberg.github.io/"/>
  <updated>2025-01-30T18:26:11+01:00</updated>
  <id>http://jivimberg.github.io/</id>
  <author>
    <name><![CDATA[Juan Ignacio Vimberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pivot Tracing]]></title>
    <link href="http://jivimberg.github.io/blog/2022/02/14/pivot-tracing/"/>
    <updated>2022-02-14T23:02:52-08:00</updated>
    <id>http://jivimberg.github.io/blog/2022/02/14/pivot-tracing</id>
    <content type="html"><![CDATA[<p>Pivot Tracing lets users define arbitrary metrics over trace data at runtime. It does so by combining two powerful techniques:</p>

<ol>
<li>A <strong>Happen-Before operator</strong> that allows users to perform queries based on the causal relationship of the events.</li>
<li>The ability to <strong>instrument code dinamically</strong> without having to redeploy.</li>
</ol>


<!--more-->


<h2>How it works</h2>

<p>Say we have an instrumented system we want to troubleshoot:</p>

<p><img class="center" src="/images/posts/2022-02-14/pt1.jpg" width="700"></p>

<p>The first step is to define <strong>Tracepoints</strong> ⓵. Tracepoints are pointers to the source code where the code will be instrumented to collect metrics. They are instructions on <em>where</em> and <em>how</em> to modify the system to extract the required information. You can think of them as <a href="https://docs.spring.io/spring-framework/docs/3.0.x/spring-framework-reference/html/aop.html"><em>pointcuts</em> from aspect-oriented programming</a>. They can refer to any arbitrary interface or method signature or be inserted at specific line numbers.</p>

<p>Tracepoints export variables that can be used to write the queries, such as method arguments or local variables. Some default variables are provided out-of-the-box, such as: <em>host</em>, <em>timestamp</em>, <em>process id</em>, <em>process name</em>, etc.</p>

<p>The cool thing about Tracepoints is that <strong>they don’t need to be defined a priori</strong>. And since defining a Tracepoint involves no code modification, <strong>new ones can be created at no cost</strong>.</p>

<p>Tracepoints are created by someone with good system knowledge (such as developers or operators). They <strong>define the vocabulary users will use to write the queries</strong>.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt2.jpg" width="700"></p>

<p>The next step is to write a query using the terms introduced by the tracepoints ⓶. A query might look something like this:</p>

<pre><code class="sql">From incr In DataNodeMetrics.incrBytesRead
Join cl In First(ClientProtocols) On cl -&gt; incr
GroupBy cl.procName
Select cl.procName, SUM(incr.delta)
</code></pre>

<p>In this example, a Tracepoint exists for the method <code>DataNodeMetrics. incrBytesRead(int delta)</code>  and another for the class <code>ClientProtocols</code>. The shown query sums the values of <code>incr.delta</code> grouped by ClientProtocol’s <code>procName</code>. The clause <code>On cl -&gt; incr</code> establishes that we’re interested in only capturing <code>cl</code> events that happened before an <code>incr</code> event. <strong>The cool thing is that these two tracepoints belong to two different services! Pivot tracing will know how to propagate the required context needed to do the join and evaluate the happens-before clause.</strong></p>

<p>Once the user submits the query, Pivot Tracing frontend compiles it to something called <strong>Advice</strong> ⓷. An Advice contains the instructions that need to be executed when a request passes through a Tracepoint to answer the query. These are the operations that can be executed as part of an Advice:</p>

<ul>
<li><strong>Observe:</strong> Creates a tuple from variables defined by a tracepoint.</li>
<li><strong>Filter:</strong> Evaluates a predicate on all tuples.</li>
<li><strong>Pack:</strong> Makes tuples available for use by later Advice</li>
<li><strong>Unpack:</strong> Retrieves one or more tuples from prior Advice</li>
<li><strong>Emit:</strong>Outputs a tuple for global aggregation.</li>
</ul>


<p>The Frontend creates these Advice and notifies the PT Agents to install them in the corresponding Tracepoints ⓸.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt3.jpg" width="700"></p>

<p>This is done by <em>weaving</em> generated code that gets executed every time a request passes through the Tracepoint.</p>

<p><img class="center" src="/images/posts/2022-02-14/weaving.png" width="700"></p>

<p>For the query shown above, Advice A1 and A2 would be generated:</p>

<p><img class="center" src="/images/posts/2022-02-14/generatedAdvices.png" width="700"></p>

<p>To do the happens-before join required by the query shown above Pivot Tracing needs to propagate the variables captured at <code>ClientProtocols Tracepoint</code> to the  <code>DataNodeMetrics Tracepoint</code>. This is done through something called <em>baggage</em> (if you’re familiar with <a href="https://opentelemetry.io/docs/concepts/signals/baggage/">OTel’s baggage</a> it’s basically the same thing).  When the request gets to the <code>ClientProtocols Tracepoint</code>, the Advice A1 <em>Observes</em> the variables and <em>Packs</em> them as part of the request baggage ⓹. Later, when the code gets to the <code>DataNodeMetrics Tracepoint</code>, the Advice A2 <em>Unpacks</em> the variables, <em>Observes</em> the value of <code>delta</code>, and <em>Emits</em> a joined tuple ⓺.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt4.jpg" width="700"></p>

<p>The tuples emitted by <code>DataNodeMetrics Tracepoint</code> ⓺ are aggregated locally and then streamed to the client over the message bus ⓻.</p>

<p>Finally, Pivot Tracing Frontend uses these tuples to answer the user’s query ⓼. And that’s how the whole thing works.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt5.jpg" width="700"></p>

<h2>The power of Happens-Before</h2>

<p>One of the most remarkable features of Pivot Tracing is the Happens-Before operator: <code>-&gt;</code>. With a Happens-Before join, users can tell the system to capture events on  <em>Tracepoint B</em> only if the request first passed through <em>Tracepoint A</em>.</p>

<p>Queries that require information from multiple services participating in a request (such as the one shown above) can’t be answered through Logs or Metrics captured on single service. By the time a metric is recorded on Service A, we no longer have the context to know where that request has come from. Distributed Tracing was created to fulfill this gap, to answer the questions Logs and Metrics couldn’t answer. <strong>Yet many of today’s most popular frameworks still lack the ability to write causal queries</strong>.</p>

<p>Pivot Tracing not only supports this, but it also does it in a cost-efficient manner by only propagating just enough context to answer a given user query. This is only possible thanks to their use of Dynamic Instrumentation.</p>

<p><img class="center" src="/images/posts/2022-02-14/happens-before.jpeg" width="500"></p>

<p>So far, we’ve used a straightforward request path for our examples: Service A -> Service B. It’s worth noting that the requests call graphs tend to be much more complex in real life. The tracepoints the user might be interested in querying could be N levels removed. The image above shows which tuples are captured by each query based on the execution graph.</p>

<h2>Pros and cons of Dynamic Instrumentation</h2>

<p>We talked about how Pivot Tracing uses AOP to dynamically instrument the code. The promise made by Dynamic Instrumentation is that developers don’t need to worry about deciding what parts of the code to instrument and which tags to record. Instead, Pivot Tracing takes care of making the required code changes to be able to answer any given query.</p>

<p>On paper, this sounds amazing. <strong>It means developers don’t need to think about observability storage costs when writing code.</strong> They don’t need to consider how traffic spikes and tag cardinality might conspire to make observability prohibitively expensive. It also removes the need to decide what parts of the code are worth measuring at design time, which is great because, by the time you’re troubleshooting an issue, you have a better idea about what kind of questions you might want to ask.</p>

<p>The cost structure makes sense too. Think about all those log lines an application spits per second and how most of the time, only one in a gazillion lines is interesting. Furthermore, Dynamic Instrumentation means the code is only instrumented as a result of a query written by the user, so any cost incurred is well justified because you can guarantee somebody is looking at the emitted data. <strong>So basically, you only pay for what you use.</strong></p>

<p>So where’s the catch?</p>

<p>We said that the Pivot Tracing only starts collecting information after the user has submitted a query. <strong>This means you only start observing the system after the issue has already occurred</strong>. If you’re lucky, it’ll happen again soon, and you’ll be able to gain some insight from the provided query. Now, <em>this is a huge if</em>. Some of the most complex bugs are those that show up every now and then and are only reproducible under rare conditions. For these bugs dynamic instrumentation might not help much. A long running query will have to be installed in place so that information can be collected the next time it happens.</p>

<p><strong>Another downside is that there’s no historical data to use as a baseline.</strong> Say users are reporting some page is slow. You can run a query and get a latency breakdown across services, but how do you know if what you’re seeing is an anomaly? Was the system this slow a week ago? Does service X always take more than 3 seconds to answer? Has the request time been affected by the deployment to service Y we made two days ago? These questions can’t be answered without stored observability data to refer to.</p>

<p>The good news is that the choice between static and dynamic instrumentation doesn’t have to be binary. In reality, you’ll want to have a mix of statically instrumented known queries (this is where your dashboard and alerts data will come from), <em>AND</em> some headroom for dynamically instrumented queries for troubleshooting. The final cost ends up being the sum of both types of instrumentation. The cost for static instrumentation is driven by storage cost, whereas the dynamic instrumentation cost is a function of emission cost which depends on the query being executed. Of course, both are directly affected by traffic volume.</p>

<p>Speaking of costs, one of the dangerous aspects of dynamic instrumentation is that the cost is directly influenced by the query provided. <strong>The paper describes no way of understanding and evaluating these costs, so they remain virtually hidden from the user writing the query</strong>. A given query could result in many kilobytes of Advice being transmitted as baggage across N services. Consider, once more, the query:</p>

<pre><code class="sql">From incr In DataNodeMetrics.incrBytesRead
Join cl In First(ClientProtocols) On cl -&gt; incr
GroupBy cl.procName
Select cl.procName, SUM(incr.delta)
</code></pre>

<p>This is a typical example of a happens-before query. The clause <code>cl -&gt; incr</code> means we only care about <code>incr</code> events if the request first passes through <code>cl</code>. A user without knowledge of how Pivot Tracing works might think that the cost of this query is proportional to the number of requests it matches, but this would be completely wrong. To answer this query, Pivot Tracing must pack <em>Advice</em> in all requests that go through <code>cl</code>. This baggage is then transmitted across <em>all</em> services downstream from <code>cl</code>. This needs to be done because the system doesn’t know a priori which of those requests will then go through <code>incr</code>.</p>

<p><img class="center" src="/images/posts/2022-02-14/instrumentation-comp.jpeg" width="700"></p>

<p>Any given query might affect the baggage content passed across <em>N</em> number of services, and there’s no way of knowing which services will be affected at query time.</p>

<p>To make things even more complicated, the capacity reserved to answer tracing queries is shared across all system users. This introduces a new set of challenges around administering said capacity. Deciding which team should have priority to run a query is not something you want to be dealing with during an incident.</p>

<p>The paper describes a series of query optimizations to keep the baggage level small. It also mentions a possible workaround to avoid packing too many tuples into the baggage, which consists of emitting all tuples instead of packing them. However, even with this workaround, some information still needs to be packed into the request context to be able to reconstruct the causality.</p>

<p>One last concern regarding dynamic instrumentation is that, by definition, <strong>a given query modifies the system being observed</strong>. If you’ve ever tried troubleshooting a bug only to discover that it doesn’t happen when the application is run on debug mode, you know how annoying this could be. In certain situations running the query might even be counter-productive. For example, consider a user troubleshooting an incident on app performance. Running a Pivot Tracing query will further add extra overhead to the system, making things even worse.</p>

<p>Despite all these pitfalls, I still believe Dynamic Instrumentation is a valuable tool and, I’d love to see it adopted in mainstream distributed tracing frameworks. The ability to write queries about any part of the code without having to manually instrument it’s just too good to pass on. We shouldn’t treat it as the primary instrumentation strategy but as a secondary method of answering complex, more specific queries crafted to troubleshoot particular incidents. Engineers designing these tools need to take extra care and put the proper safeguards in place to avoid users shooting themselves in the foot.</p>

<p><img class="right-fill" src="/images/signatures/signature3.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sifter: Scalable Sampling for Distributed Tracing]]></title>
    <link href="http://jivimberg.github.io/blog/2021/07/28/sifter-scalable-sampling-for-distributed-traces/"/>
    <updated>2021-07-28T08:39:37-07:00</updated>
    <id>http://jivimberg.github.io/blog/2021/07/28/sifter-scalable-sampling-for-distributed-traces</id>
    <content type="html"><![CDATA[<p>Distributed tracing can be ridiculously expensive if you try to trace a hundred percent of requests. A common technique to reduce costs is to sample only a small portion of the traffic. But naive sampling techniques like <em>uniform sampling</em> will inevitably capture more common-case executions and might miss the more interesting edge cases. Instead, <strong><a href="https://dl.acm.org/doi/10.1145/3357223.3362736">Sifter’s approach</a> is to bias sampling decisions towards outliers and anomalous traces.</strong> This way, anomalous traces have a higher chance of being sampled, and the more uninteresting traces are discarded.</p>

<!--more-->


<h2>How it works</h2>

<p>Sifter uses the incoming stream of traces to <strong>builds a model that approximates the system’s common-case behavior</strong>. To make a sampling decision, it checks how well the model matches the incoming trace. Sampling is biased toward traces that are <em>not</em> well approximated (outliers and anomalous traces).</p>

<p><img class="center" src="/images/posts/2021-07-28/full-pipeline.png"></p>

<h3>The model</h3>

<p>Sifter uses a 2-layer neural network that models the probability of an event occurring given its immediate predecessors and successors. To create this model, they first extract all <em>N</em>-length paths for each trace.</p>

<p><img class="center" src="/images/posts/2021-07-28/paths.png"></p>

<p>For a given path (p_0<em>, …, p_N</em>), the neural network predicts the probability of p_N/2_ given its surrounding events p_0<em>, … , p_N</em> (excluding  p_N/2_). The model gives better predictions for paths that are seeing more frequently in the input dataset.</p>

<p><img class="center" src="/images/posts/2021-07-28/neural-network.png"></p>

<h3>Sampling</h3>

<p>To make a sampling decision, Sifter first extracts all <em>N</em>-length paths from traces and feeds that into the model. It then uses the output to calculate the prediction&rsquo;s <em>loss</em>, that is the difference between the predicted and actual events. <strong>The higher the loss, the more “interesting” the trace is.</strong></p>

<p>The <em>loss</em> is then weighted against the loss of the <em>k</em> most recent traces.</p>

<p><img class="center" src="/images/posts/2021-07-28/formula-loss.png"></p>

<p>If all traces have the same <em>loss</em>, then the trace is sampled uniformly at random with probability <em>α</em>. Otherwise, the probability is calculated using this formula:</p>

<p><img class="center" src="/images/posts/2021-07-28/formula-sampling.png"></p>

<p>Traces with the lowest <em>loss</em> will have a sampling probability of zero. Traces with the highest loss will have the highest sampling probability.</p>

<h3>Updating the model</h3>

<p>Sifter does a gradient descent back-propagation pass on the model for every trace seen to keep the model up-to-date. It also adds the trace’s loss into the window of <em>k</em> most recent traces and pops the oldest value. Note that <strong>this happens on every trace regardless of whether it is sampled or not.</strong></p>

<h2>Sifter’s Properties</h2>

<p>The following list is a summary of Sifter’s design choices and their consequences:</p>

<ul>
<li><strong>Sifter’s operates online over a continuous stream of traces.</strong>  The overhead added by sampling is in the order of milliseconds. This latency is affected only by the trace size and the number of unique events in a trace. It is independent of the workload volume and the number of previously sampled traces.</li>
<li><strong>Sifter is not feature-based.</strong> The model used to identify interesting traces does not require any feature engineering from developers. As a consequence, Sifter’s approach is not limited to what developers consider interesting a priori. Instead, interesting features on the sampled traces can be discovered rather than engineered.</li>
<li>As a corollary to the previous point, <strong>Sifter can automatically adapt to changes in traffic over time.</strong> The model is updated with every incoming trace, and the sampling probability is adjusted as the workload distribution changes.</li>
<li>The amount of memory needed to keep Sifter’s model is a function of <em>N</em> the path size, <em>E</em> the label vocabulary, and <em>k</em> the number of most recently seen traces we consider. The total size is constant with respect to the workload volume and the number of sampling traces.</li>
<li><strong>Sifter does not require any pre-training or manual configuration before operation.</strong> On bootstrap, it’ll start making random sampling decisions while learning the model’s system behavior with every new trace.</li>
<li>Sifter’s model converts traces into a <em>directed acyclic graph</em> of labels based on event origin, i.e. source file and line number. It does not consider other span features like timing, tags, annotations, etc.</li>
</ul>


<p> <img class="right-fill" src="/images/signatures/signature3.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[About Deploying on Fridays]]></title>
    <link href="http://jivimberg.github.io/blog/2021/01/22/about-deploying-on-fridays/"/>
    <updated>2021-01-22T08:55:35-08:00</updated>
    <id>http://jivimberg.github.io/blog/2021/01/22/about-deploying-on-fridays</id>
    <content type="html"><![CDATA[<p>Common knowledge says that you don’t deploy on Friday if you want to have a peaceful weekend. Yet, some people will tell you that if you’re not comfortable deploying every day of the week, you’re doing it wrong. They’ll say that deploying shouldn’t be scary and that you probably don’t have enough tests. So, which one is it?</p>

<!--more-->


<p><img class="center" src="/images/posts/2021-01-22/homer-deploying.png" width="350"></p>

<h2>The problem with Fridays</h2>

<p>Fridays are the last day of the working week, and for many teams, it might also be the end of the sprint/cycle/iteration. Explicit or not, there’s a deadline looming between the developer and the weekend. We invested long hours on this new feature, and we just want to push it out the door, close the ticket, and come back to a clean slate on Monday. Nobody likes heading into the weekend feeling like a fraud because they couldn’t complete the planned tasks in time. The bigger the task, the more eager we are to close it.</p>

<p>So what do we do? We rush it. Maybe we skip testing in a staging environment or turn a blind eye to that flaky test that always fails, and <em>we ship it</em>. We feel the weight being lifted off our shoulders as we click deploy, and we head out the door smiling, ready to enjoy the weekend.</p>

<p>Only somebody will have to clean up our mess when things go south.</p>

<h2>When is a task completed?</h2>

<p>To understand how to combat the danger of Fridays, let’s consider the <em>software lifecycle</em>. That is, the things that happen between <em>“I have to implement feature X”</em> and  <em>“<em>people using it in production</em>”</em>.</p>

<p><img class="center" src="/images/posts/2021-01-22/software-lifecycle1.jpeg" width="700"></p>

<p>From the developer&rsquo;s point of view, when would you say that feature X is <em>Done</em>? Is it once the branch is merged? Is it after the code is deployed? The answer depends on what your team considers the developer’s responsibility. On teams doing <a href="https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249">Full Cycle Development</a>, the same person writing the code is the one that will test, operate and deploy the service. Which means that <strong>the feature can only be marked as Done once it’s being used in production.</strong></p>

<p>Deploying is only one step, after which you <strong>release</strong> the change to some (or all) of your users and <strong>observe</strong> that it is working as intended. <em><em>Spoiler Alert</em></em> Sometimes it won’t be, and you’ll have to go back to the code and add little fixes here and there.</p>

<p>Treating deployments as just another phase of the software lifecycle enables <a href="https://www.infoq.com/articles/observability-driven-development/">Observability Driven Development</a>. Letting developers <em>see</em> how their code behaves in production before closing the task.</p>

<h2>Optimizing the feedback loops</h2>

<p>The software lifecycle diagram shown above is an over-simplification. In reality, the flow is never a straight line. Every step can send you back to a previous step: Started coding and found a flaw in the design, go back to design  🔙, a test started failing, go back to develop 🔙, etc.</p>

<p><img class="center" src="/images/posts/2021-01-22/software-lifecycle2.jpeg" width="700"></p>

<p><a href="https://martinfowler.com/articles/developer-effectiveness.html">The secret sauce to a highly efficient team is keeping these feedback loops as short as possible</a>. If your tests take thirty minutes to run, by the time you see the failure, you’re already deep in some other task (or worse, on your 9th YouTube video).</p>

<p>Deploying is no different. If it takes months for your code to reach production, by the time your users start using your new feature (and uncovering bugs), you have already moved on to something else. You no longer have the context fresh on your mind, and you barely recall the details and the design decisions taken at the time (which is why <a href="https://jivimberg.io/blog/2020/12/26/documenting-decisions/">you should be documenting those decisions</a>).</p>

<p><img class="center" src="/images/posts/2021-01-22/software-lifecycle3.jpeg" width="700"></p>

<h2>How to make deployments less scary</h2>

<p><strong>The single most important change you can make to have less scary deployments, is to deploy small changes. Ideally, deploying one change at a time. One Pull Request ➡️ one Deploy.</strong> This will inevitably lead to more deployments because now you might have to do ten deployments to match your big dump releases of the past. This is good! The more you deploy, the less scary it is.</p>

<p>Deploying small changes also gives you better visibility into how your code is affecting the service. <strong>By releasing one change at a time, developers can use telemetry to observe how the code behaves in production and spot bugs before your users do</strong>. In contrast, if you batch multiple PRs in a single deploy, you might have a harder time figuring out which of the changes caused the issue. You might even have to convince the developer that what caused the issue is indeed their commit and not somebody else’s bundled together in the same release. You can avoid all this hassle by following the “one PR ➡️ one Deploy” rule.</p>

<p>And the benefits don’t end there! Smaller changes also mean shorter code reviews. It’s easier for developers reviewing your code to spot bugs in a small PR than in a huge one that modifies hundreds of lines. This is another way in which smaller changes bring better code quality.</p>

<p><img class="center" src="/images/posts/2021-01-22/huge-pr.jpeg" width="400"></p>

<p>Last but not least, smaller PRs produce short-lived branches, reducing the number of merge conflicts one has to deal with.</p>

<h2>Deploying != Releasing</h2>

<p>For this approach to work, you need to trust devs with the keys to production. Depending on how your team operates, this might sound risky. <em>Are you saying developers can put code in production whenever they want?</em> Yes! That’s exactly what we’re advocating for. As pointed out earlier, this will mean more deploys, but it’s generally safer than the humongous release approach. Even if it feels like you lose control of what goes out the door. Yes, you’ll be deploying bugs from time to time, but the blast radius is limited, and the change is easier to rollback.</p>

<p><strong>Now, this doesn’t mean that all users are immediately able to see the new changes as soon as you deploy.</strong> The terms <em>deploy</em> and <em>release</em> are sometimes used interchangeably, so let’s define how we’ll use them here:</p>

<ul>
<li><strong>Deploy:</strong> Put a new version of the code in production.</li>
<li><strong>Release:</strong> Make some functionality available to users.</li>
</ul>


<p>You can (and should) still control at what rate new functionality is released to users. You might use a rollout strategy where only a subset of power-users get to see what you’re working on and provide feedback before the feature is released to a broader audience. Or, you might want to start by observing how it works on a small percentage of users and then gradually roll out to everyone else. You can achieve this by hiding the new code behind <a href="https://martinfowler.com/articles/feature-toggles.html">Feature Flags</a> and have it conditionally enabled. This provides the extra benefit of being able to enable and disable the code with a simple configuration change (without requiring a deploy), should a critical bug be found.</p>

<h2>It’s not about testing</h2>

<p>One common argument from the <em>“deploy on Fridays”</em> camp is about testing. It goes like this:</p>

<blockquote><p>If you’re scared of deploying on a Friday, it means you either don’t have enough tests or your tests are not good enough.</p></blockquote>

<p>I don’t buy it.</p>

<p>No matter how many tests you have, how good your coverage is, you can’t be sure you’re not releasing a bug. In the words of Dijkstra: <em> Program testing can be used to show the presence of bugs, but never to show their absence.</em></p>

<p>Most automated tests are about validating the scenarios the dev can come up with during development. They don’t account for things we can’t predict<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. So most testing is limited by the imagination of the person writing the test.</p>

<p>Furthermore, our tests usually run in a fake environment where many of the components are mocked. From service stubs to in-memory databases, we use every trick in the book to <a href="https://jivimberg.io/blog/2020/07/27/effective-testing-reducing-non-determinism/">reduce non-determinism from our tests</a>, but this comes at the cost of test fidelity. Our tests no longer accurately represent what happens in production. This is why we need observability and <a href="https://copyconstruct.medium.com/testing-in-production-the-safe-way-18ca102d0ef1">testing in production</a>. This is why we need to <em>deploy</em> and <em>observe</em> to ensure our code is working as intended.</p>

<h2>Conclusion</h2>

<p>To sum up, deploying is just an additional step of the software lifecycle. <strong>You should deploy any day of the week, providing you’re willing to stick around to observe how your code behaves in production. </strong> If you just want to deploy, and run home to start the weekend, then maybe <em>don’t</em>. Because no matter how many tests you have, you haven’t seen your code running in a real environment yet. In the future, we might have DevOps AI to observe and rollback our changes automatically if something looks weird. Until then, though, you’re on the hook for making sure your code is working as intended. <em>Especially on Fridays.</em></p>

<p>Piecemeal deployments will help you release faster and will improve your code quality. The idea is counter-intuitive, but it works: you have to do the scary thing over and over until it becomes an uninteresting event. Releasing frequently will help you catch bugs sooner and will make your team more efficient.</p>

<hr />

<p>Many of the ideas in this post are inspired by the podcasts <a href="https://www.heavybit.com/library/podcasts/o11ycast/">Oll1cast</a> and <a href="https://maintainable.fm/">Maintainable</a>, as well as the books <a href="https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791">Software Engineering at Google</a> and <a href="https://www.oreilly.com/library/view/distributed-tracing-in/9781492056621/">Distributed Tracing in Practice</a>. If you enjoy these topics, go check them out.</p>

<p> <img class="right-fill" src="/images/signatures/signature7.png" width="200" title="‘My signature’" ></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>That’s what exploratory testing is for, and it’s a creative endeavor that doesn’t scale linearly with code.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
</feed>
