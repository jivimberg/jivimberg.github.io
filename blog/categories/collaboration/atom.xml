<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Collaboration | Coding Forest]]></title>
  <link href="http://jivimberg.github.io/blog/categories/collaboration/atom.xml" rel="self"/>
  <link href="http://jivimberg.github.io/"/>
  <updated>2023-09-19T00:49:57-07:00</updated>
  <id>http://jivimberg.github.io/</id>
  <author>
    <name><![CDATA[Juan Ignacio Vimberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pulling an Inverse Conway Maneuver at Netflix]]></title>
    <link href="http://jivimberg.github.io/blog/2023/09/04/the-inverse-conway-maneuver/"/>
    <updated>2023-09-04T10:07:02-07:00</updated>
    <id>http://jivimberg.github.io/blog/2023/09/04/the-inverse-conway-maneuver</id>
    <content type="html"><![CDATA[<p>When I first joined the Netflix Platform team circa 2020, the Observability offering was composed of a series of tools serving different purposes. There was <a href="https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a">Atlas</a> for metrics, <a href="https://netflixtechblog.com/edgar-solving-mysteries-faster-with-observability-e1a76302c71f">Edgar</a> for distributed tracing, Radar for Logs and <a href="https://netflixtechblog.com/improved-alerting-with-atlas-streaming-eval-e691c60dc61e">Alerts</a>, <a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a> for dashboards, <a href="https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba">Telltale</a> for app health, etc. It was a portfolio of about 20 different apps. Big and small, ranging from business-specific tools to analyze playback sessions to <a href="https://netflixtechblog.com/java-in-flames-e763b3d32166">low-level tools for CPU profiling</a>.</p>

<!--more-->


<p>The Observability org was composed of three different teams. Each team had a mix of <em>front-end</em>, <em>back-end</em> and <em>full-stack</em> engineers. We also had one <em>designer</em> and one <em>PM</em> shared across the three teams. Each team was further subdivided into sub-teams of two to four engineers working on a specific sub-domain.</p>

<p><img class="center" src="/images/posts/2023-09-04/observability-org-structure-1.jpg" width="750"></p>

<p>It was no coincidence that this org structure produced <em>a set of independent apps</em>. That&rsquo;s the kind of architecture we&rsquo;d expect based on Conway&rsquo;s Law:</p>

<blockquote><p>Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization&rsquo;s communication structure.</p>

<p><em>Melvin E. Conway</em></p></blockquote>

<p>Simply put, the system&rsquo;s architecture will be shaped like the org that produced it. This is because, to build a complex system, people must communicate to ensure the different pieces fit well together. <strong>Therefore, the design that emerges will be a map of the communications paths in the organization.</strong></p>

<p>Netflix&rsquo;s approach to building software further intensified this. Netflix embraces the <a href="https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249">Full Cycle Development</a>; this means teams are fully responsible for all the stages of the software lifecycle, from Design to Operate and Support.</p>

<p><img class="center /images/posts/2023-09-04/full-developer-lifecycle.png 480 <code>The empowered full cycle" src="developer</code>"></p>

<p>We were organized as <a href="https://noeldykes.medium.com/what-we-can-learn-from-the-netflix-culture-deck-as-business-leaders-ed35ed8c0689#:~:text=Highly%20Aligned%2C%20Loosely%20Coupled,are%20clear%2C%20understood%20and%20focused.">highly aligned, loosely coupled</a> teams with a high level of independence. ICs wholly owned every aspect of their work, from tech stack choices to which ticketing platform to use to track bugs. Netflix provides a recommended set of tools (known as &ldquo;paved path&rdquo;) but doesn&rsquo;t mandate their adoption. Each team is free to pick whatever tools and practices suit them best.</p>

<blockquote><p>Netflix has a “paved road” set of tools and practices that are formally supported by centralized teams. We don’t mandate adoption of those paved roads but encourage adoption by ensuring that development and operations using those technologies is a far better experience than not using them.</p>

<p><em>Extract from <a href="https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249">Full Cycle Development blog post</a></em></p></blockquote>

<p>This produced a set of heterogeneous apps to serve the observability needs of the company. Each team would focus on its own sub-domain and individual products to deliver the best possible experience. And users were happy with the result. At least for a while&hellip;</p>

<p>By 2020, we started hearing a new kind of complaint. Users were beginning to get frustrated with the disjointed experience we provided for troubleshooting. <strong>Debugging a particular issue required them to replicate the same query across multiple tools and jump between tabs to assemble the pieces.</strong> To troubleshoot effectively, users had to be proficient with each tool and know when to use one or the other. To make matters worse, the different apps' documentation was scattered across multiple wikis, and we hadn&rsquo;t done a great job teaching users about new tools and features. It also didn&rsquo;t help that each app implemented its own base components (such as date pickers or query builders) with subtle variations. The required functionality was there, but it was only accessible to power users, and even then, having a comprehensive view of an issue took quite a bit of effort.</p>

<p>Our knee-jerk reaction to the feedback was adding deep links across apps so that users could jump to a different tool, taking the query context with them. This would make it easier to flow from one tool to the next when required. To make this happen, we had to start talking across the teams to align on a standard to send and receive contextual information through the links. Even something as trivial as this took us multiple meetings to agree on a standard that&rsquo;d satisfy the needs of all apps of the portfolio.</p>

<p><img class="center" src="/images/posts/2023-09-04/observability-org-structure-2.jpg" width="750"></p>

<p>Soon, we realized links were not going to cut it. We were also getting frustrated with how long it took us to coordinate these changes across teams. The links made the fact that we had some overlap between tools quite obvious. For example, you could pull all log messages for a given request in Edgar, but you&rsquo;d see them on Edgar&rsquo;s own log viewer component, which wasn&rsquo;t as powerful at Radar&rsquo;s. With deep linking, users could click the log message and see it on Radar, but at the cost of losing the request context, which only made sense on Edgar.</p>

<p>We came to the realization that if we wanted to provide a cohesive observability story we&rsquo;d need a single application that lets users interrogate multiple data sets at once. A place where they could observe their systems from different angles without having to jump through various hoops.</p>

<p><img class="center" src="/images/posts/2023-09-04/observability-org-structure-3.jpg" width="750"></p>

<p>We knew what kind of architecture we were going for, and based on Conway&rsquo;s Law we knew it would be hard to achieve it with the current org structure. So before we even discussed how we would implement anything, before we even knew if we were creating a new tool, a whole platform, or buying some ready-made solution, management made one decision. They pulled an Inverse Conway Maneuver and re-orged the teams. <strong>They re-shape the org structure to match the design solution we were going for, creating the communication paths required (and severing the ones not needed) to facilitate the work.</strong> And that is how the Explore team was born.</p>

<p><img class="center" src="/images/posts/2023-09-04/observability-org-structure-4.jpg" width="750"></p>

<p>With this new structure, communication was optimized to produce a unified experience that&rsquo;d include all existing features across logs, metrics, traces, alerts, and dashboards. The trade-off was that now the back-end for any given vertical lived on a different team. <strong>This implies less communication between front-end and back-end engineers, making working on features requiring alignment between both parts slower.</strong> Management considered before doing the re-org, and decided the cost was acceptable because the goal of unification was a higher priority on our roadmap.</p>

<p>The takeaways here are:</p>

<ol>
<li>The org structure limits the design solutions an org can produce for a given system&rsquo;s architecture. This is Conway&rsquo;s Law.</li>
<li>If you know which architecture you aim for, you can adapt the org structure to facilitate arriving at your goal. This is known as the Inverse Conway Maneuver.</li>
<li>You might need different org configurations during the lifetime of a system, depending on what your goals are at the time.</li>
<li>It&rsquo;s important to consider the trade-offs of choosing any given org structure, understanding which communication paths are being optimized and which ones de-prioritized, and how that affects the flow of work.</li>
</ol>


<p>If you find this topic interesting, check out the book <a href="https://teamtopologies.com/">Team Topologies</a> by <em>Matthew Skelton</em> and <em>Manuel Pais</em>.</p>

<p><img class="right-fill" src="/images/signatures/signature5.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Platform Teams Get Shit Done]]></title>
    <link href="http://jivimberg.github.io/blog/2023/07/28/how-platform-teams-get-shit-done/"/>
    <updated>2023-07-28T00:43:59-03:00</updated>
    <id>http://jivimberg.github.io/blog/2023/07/28/how-platform-teams-get-shit-done</id>
    <content type="html"><![CDATA[<p><a href="https://thepete.net/">Pete Hodgson</a> explored the different ways in which a Platform team works with other teams to get shit done in <a href="https://martinfowler.com/articles/platform-teams-stuff-done.html">this article</a>. I thought it was interesting to see how collaboration changes based on the type of work, so I put together this visual summary to compare and contrast each type of interaction.</p>

<!--more-->


<p><a href="/images/posts/2023-07-28/how-platform-teams.png"><img class="center" src="/images/posts/2023-07-28/how-platform-teams.png" width="600"></a>
<em class="img-caption">Click image to enlarge</em></p>

<p>I added a few things here and there, but most of the stuff is taken from the <a href="https://martinfowler.com/articles/platform-teams-stuff-done.html">original article</a>, so if you care about this topic I recommend you check it out!</p>

<p>I found particularly interesting the realization that migrations are hard because <strong>the team that owns the code that needs changing is not the one driving the migration.</strong> This creates a situation of misaligned incentives and makes it a socio-technical problem. The article describes the different ways the teams can collaborate to get it done, but it’s also important to understand the tools a platform team has to remove this friction in the first place, things like <a href="https://medium.com/nerd-for-tech/microservice-design-pattern-sidecar-sidekick-pattern-dbcea9bed783">sidecars</a>, <a href="https://konghq.com/learning-center/service-mesh/what-is-a-service-mesh#:~:text=Service%20mesh%20is%20a%20technology,it%20can%20be%20managed%20independently.">meshes</a> and <a href="https://blog.thepete.net/blog/2020/09/25/service-templates-service-chassis/">service chasis</a>.</p>

<p>I also included a section on how Google does Large-Scale Changes (LSC). They created a tool that allows anybody to submit Large-Scale Changes that are applied across the whole codebase. They advocate for the approach of centralizing the migration, to the point where most changes are reviewed by a single expert, and local teams hold no veto power over the LSC. They rely on code analysis and transformation tools to write the LSC and have a test infrastructure to automatically run all tests that a given change might affect in an efficient manner. To read more about their approach refer to <a href="https://www.amazon.com/Software-Engineering-Google-Lessons-Programming/dp/1492082791">Software Engineering at Google</a> Chapter 14.</p>

<p><img class="right-fill" src="/images/signatures/signature7.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
</feed>
