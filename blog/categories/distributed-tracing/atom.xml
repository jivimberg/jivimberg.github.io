<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Distributed-tracing | Coding Forest]]></title>
  <link href="http://jivimberg.github.io/blog/categories/distributed-tracing/atom.xml" rel="self"/>
  <link href="http://jivimberg.github.io/"/>
  <updated>2023-05-01T19:42:54-07:00</updated>
  <id>http://jivimberg.github.io/</id>
  <author>
    <name><![CDATA[Juan Ignacio Vimberg]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pivot Tracing]]></title>
    <link href="http://jivimberg.github.io/blog/2022/02/14/pivot-tracing/"/>
    <updated>2022-02-14T23:02:52-08:00</updated>
    <id>http://jivimberg.github.io/blog/2022/02/14/pivot-tracing</id>
    <content type="html"><![CDATA[<p>Pivot Tracing lets users define arbitrary metrics over trace data at runtime. It does so by combining two powerful techniques:</p>

<ol>
<li>The ability to <strong>dynamically instrument code</strong> without having to redeploy.</li>
<li>A <strong>Happen-Before operator</strong> that allows users to perform queries based on the causal relationship of the events.</li>
</ol>


<!--more-->


<h2>How it works</h2>

<p>Say we have an instrumented system we want to troubleshoot:</p>

<p><img class="center" src="/images/posts/2022-02-14/pt1.jpg" width="700"></p>

<p>The first step is to define <strong>Tracepoints</strong> ⓵. Tracepoints are pointers to the source code where the code will be instrumented to collect metrics. They are instructions on <em>where</em> and <em>how</em> to modify the system to extract the required information. You can think of them as <a href="https://docs.spring.io/spring-framework/docs/3.0.x/spring-framework-reference/html/aop.html"><em>pointcuts</em> from aspect-oriented programming</a>. They can refer to any arbitrary interface or method signature and can also be inserted at specific line numbers.</p>

<p>Tracepoints export variables that can be used to write the queries, such as method arguments or local variables. Some default variables are provided out-of-the-box such as: <em>host</em>, <em>timestamp</em>, <em>process id</em>, <em>process name</em>, etc.</p>

<p>The cool thing about Tracepoints is that <strong>they don’t need to be defined a priori</strong>. And since defining a Tracepoint involves no code modification, <strong>new ones can be created at no cost</strong>.</p>

<p>Tracepoints are created by someone with good knowledge of the system (such as developers or operators). They <strong>define the vocabulary users will use to write the queries</strong>.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt2.jpg" width="700"></p>

<p>The next step is to write a query using the terms introduced by the tracepoints ⓶. A query might look something like this:</p>

<pre><code class="sql">From incr In DataNodeMetrics.incrBytesRead
Join cl In First(ClientProtocols) On cl -&gt; incr
GroupBy cl.procName
Select cl.procName, SUM(incr.delta)
</code></pre>

<p>In this example a Tracepoint exists for the method <code>DataNodeMetrics. incrBytesRead(int delta)</code>  and another one for the class <code>ClientProtocols</code>. The shown query sums the values of <code>incr.delta</code> grouped by ClientProtocol’s <code>procName</code>. The clause <code>On cl -&gt; incr</code> establishes that we’re interested in only capturing <code>cl</code> events that happened before a <code>incr</code> event. <strong>The cool thing is that these two tracepoints belong to two different services! Pivot tracing will know how to propagate the required context needed to do the join and evaluate the happens-before clause.</strong></p>

<p>Once the user submits the query, Pivot Tracing frontend compiles it to something called <strong>Advice</strong> ⓷. An Advice contains the instructions that need to be executed when a request passes through a Tracepoint to answer the query. These are the operations that can be executed as part of an Advice:</p>

<ul>
<li><strong>Observe:</strong> Creates a tuple from variables defined by a tracepoint.</li>
<li><strong>Filter:</strong> Evaluates a predicate on all tuples.</li>
<li><strong>Pack:</strong> Makes tuples available for use by later Advice</li>
<li><strong>Unpack:</strong> Retrieves one or more tuples from prior Advice</li>
<li><strong>Emit:</strong>Outputs a tuple for global aggregation.</li>
</ul>


<p>The Frontend creates these Advices and notifies the PT Agents to install them in the corresponding Tracepoints ⓸.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt3.jpg" width="700"></p>

<p>This is done by <em>weaving</em> generated code that gets executed every time a requests passes through the Tracepoint.</p>

<p><img class="center" src="/images/posts/2022-02-14/weaving.png" width="700"></p>

<p>For the query shown above Advices A1 and A2 would be generated:</p>

<p><img class="center" src="/images/posts/2022-02-14/generatedAdvices.png" width="700"></p>

<p>To do the happens-before join required by the query shown above Pivot Tracing needs to propagate the variables captured at <code>ClientProtocols Tracepoint</code> to the  <code>DataNodeMetrics Tracepoint</code>. This is done through something called <em>baggage</em> (if you’re familiar with <a href="https://opentelemetry.io/docs/concepts/signals/baggage/">OTel’s baggage</a> it’s basically the same thing).  When the request gets to the <code>ClientProtocols Tracepoint</code> the Advice A1 <em>Observes</em> the variables and <em>Packs</em> them as part of the request baggage ⓹. Later, when the code gets to the <code>DataNodeMetrics Tracepoint</code> the Advice A2 <em>Unpacks</em> the variables, <em>Observes</em> the value of <code>delta</code> and <em>Emits</em> a joined tuple ⓺.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt4.jpg" width="700"></p>

<p>The tuples emitted by <code>DataNodeMetrics Tracepoint</code> ⓺ are aggregated locally and then streamed to the client over the message bus ⓻.</p>

<p>Finally, Pivot Tracing Frontend uses this tuples to answer the user’s query ⓼. And that’s how the whole thing works.</p>

<p><img class="center" src="/images/posts/2022-02-14/pt5.jpg" width="700"></p>

<h2>The power of Happens-Before</h2>

<h2>The pros and cons of Dynamic Instrumentation</h2>

<p>Talk about:
*  Dynamic Instrumentation.
* Happens Before. Typical example of grouping metrics of service B by tags provided on service A (here A -> B)</p>

<hr />

<p>TODO</p>

<p><img class="right-fill" src="/images/signatures/signature3.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sifter: Scalable Sampling for Distributed Tracing]]></title>
    <link href="http://jivimberg.github.io/blog/2021/07/28/sifter-scalable-sampling-for-distributed-traces/"/>
    <updated>2021-07-28T08:39:37-07:00</updated>
    <id>http://jivimberg.github.io/blog/2021/07/28/sifter-scalable-sampling-for-distributed-traces</id>
    <content type="html"><![CDATA[<p>Distributed tracing can be ridiculously expensive if you try to trace a hundred percent of requests. A common technique to reduce costs is to sample only a small portion of the traffic. But naive sampling techniques like <em>uniform sampling</em> will inevitably capture more common-case executions and might miss the more interesting edge cases. Instead, <strong><a href="https://dl.acm.org/doi/10.1145/3357223.3362736">Sifter’s approach</a> is to bias sampling decisions towards outliers and anomalous traces.</strong> This way, anomalous traces have a higher chance of being sampled, and the more uninteresting traces are discarded.</p>

<!--more-->


<h2>How it works</h2>

<p>Sifter uses the incoming stream of traces to <strong>builds a model that approximates the system’s common-case behavior</strong>. To make a sampling decision, it checks how well the model matches the incoming trace. Sampling is biased toward traces that are <em>not</em> well approximated (outliers and anomalous traces).</p>

<p><img class="center" src="/images/posts/2021-07-28/full-pipeline.png"></p>

<h3>The model</h3>

<p>Sifter uses a 2-layer neural network that models the probability of an event occurring given its immediate predecessors and successors. To create this model, they first extract all <em>N</em>-length paths for each trace.</p>

<p><img class="center" src="/images/posts/2021-07-28/paths.png"></p>

<p>For a given path (p_0<em>, …, p_N</em>), the neural network predicts the probability of p_N/2_ given its surrounding events p_0<em>, … , p_N</em> (excluding  p_N/2_). The model gives better predictions for paths that are seeing more frequently in the input dataset.</p>

<p><img class="center" src="/images/posts/2021-07-28/neural-network.png"></p>

<h3>Sampling</h3>

<p>To make a sampling decision, Sifter first extracts all <em>N</em>-length paths from traces and feeds that into the model. It then uses the output to calculate the prediction&rsquo;s <em>loss</em>, that is the difference between the predicted and actual events. <strong>The higher the loss, the more “interesting” the trace is.</strong></p>

<p>The <em>loss</em> is then weighted against the loss of the <em>k</em> most recent traces.</p>

<p><img class="center" src="/images/posts/2021-07-28/formula-loss.png"></p>

<p>If all traces have the same <em>loss</em>, then the trace is sampled uniformly at random with probability <em>α</em>. Otherwise, the probability is calculated using this formula:</p>

<p><img class="center" src="/images/posts/2021-07-28/formula-sampling.png"></p>

<p>Traces with the lowest <em>loss</em> will have a sampling probability of zero. Traces with the highest loss will have the highest sampling probability.</p>

<h3>Updating the model</h3>

<p>Sifter does a gradient descent back-propagation pass on the model for every trace seen to keep the model up-to-date. It also adds the trace’s loss into the window of <em>k</em> most recent traces and pops the oldest value. Note that <strong>this happens on every trace regardless of whether it is sampled or not.</strong></p>

<h2>Sifter’s Properties</h2>

<p>The following list is a summary of Sifter’s design choices and their consequences:</p>

<ul>
<li><strong>Sifter’s operates online over a continuous stream of traces.</strong>  The overhead added by sampling is in the order of milliseconds. This latency is affected only by the trace size and the number of unique events in a trace. It is independent of the workload volume and the number of previously sampled traces.</li>
<li><strong>Sifter is not feature-based.</strong> The model used to identify interesting traces does not require any feature engineering from developers. As a consequence, Sifter’s approach is not limited to what developers consider interesting a priori. Instead, interesting features on the sampled traces can be discovered rather than engineered.</li>
<li>As a corollary to the previous point, <strong>Sifter can automatically adapt to changes in traffic over time.</strong> The model is updated with every incoming trace, and the sampling probability is adjusted as the workload distribution changes.</li>
<li>The amount of memory needed to keep Sifter’s model is a function of <em>N</em> the path size, <em>E</em> the label vocabulary, and <em>k</em> the number of most recently seen traces we consider. The total size is constant with respect to the workload volume and the number of sampling traces.</li>
<li><strong>Sifter does not require any pre-training or manual configuration before operation.</strong> On bootstrap, it’ll start making random sampling decisions while learning the model’s system behavior with every new trace.</li>
<li>Sifter’s model converts traces into a <em>directed acyclic graph</em> of labels based on event origin, i.e. source file and line number. It does not consider other span features like timing, tags, annotations, etc.</li>
</ul>


<p> <img class="right-fill" src="/images/signatures/signature3.png" width="200" title="‘My signature’" ></p>
]]></content>
  </entry>
  
</feed>
